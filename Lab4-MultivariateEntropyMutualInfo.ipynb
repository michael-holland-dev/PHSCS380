{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Multivariate Probability, Conditional Entropy and Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will be a bit more difficult than the last three labs, as it builds on probability concepts from both the beginning of the semester, as well as entropy/information theory from lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Breakdown:\n",
    "- Problem 1 - 12 Points\n",
    "- Problem 2 - 12 Points\n",
    "- Problem 3 - 12 Points\n",
    "- Problem 4 - 4 Points\n",
    "\n",
    "Total: 40 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Multivariate Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Conditional & Joint Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information is the concept that by observing one random variable, how much information have we gained by about another random variable. A qualitative example might be as follows: By observing that Pedro is eating a massive amount of icecream, how much information have we gained about Pedro's new relationship status?\n",
    "\n",
    "The equation for discrete mutual information is shown below, don't get too intimidated though, we'll break it down.\n",
    "\n",
    "$I(X,Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)$\n",
    "\n",
    "To sum up what was defined earlier, if we have two random variables, X and Y that are strongly related, by observing the X variable, we now have gained some information about Y. If this calculation is done for every possible permutation of X an Y, this is the the definition of expected information.\n",
    "\n",
    "Your task: Finish the mutual information function for a joint probability matrix that contains the mutual information for two variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mutual_information(X):\n",
    "    p_x = X.sum(axis=0) \n",
    "    p_y = X.sum(axis=1)\n",
    "    print(p_x)\n",
    "    print(p_y)\n",
    "    return mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code can go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading Code, do not edit this cell.\n",
    "assert calculate_expected_value(.25, .5, .225) == 0.45\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task: Answer the following questions\n",
    "1. What were some things you learned from this lab?\n",
    "2. What did you like about this lab?\n",
    "3. What would you improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter Your Answer Here*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
